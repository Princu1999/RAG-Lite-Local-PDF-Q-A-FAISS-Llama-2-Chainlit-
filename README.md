# RAG-Lite: Local PDF Q&A (FAISS Â· Llama-2 Â· Chainlit)

Ask questions about your own PDFsâ€”privately and locallyâ€”using a Retrieval-Augmented Generation (RAG) pipeline:
- **Ingestion**: Load PDFs from `data/`, split into chunks, and embed with `sentence-transformers/all-MiniLM-L6-v2`.
- **Vector Store**: Persist embeddings in **FAISS**.
- **LLM**: Query with **Llama-2-7B-Chat (GGML via CTransformers)** for lightweight CPU-friendly inference.
- **UI**: Chat experience powered by **Chainlit**.

> Built with LangChain + FAISS + Sentence-Transformers + CTransformers + Chainlit.

---

## âœ¨ Features
- ğŸ” **RAG**: Retrieval-augmented answers grounded in your PDFs (k=2 top matches).
- ğŸ§© **Chunking**: Recursive splitter (500 chars, 50 overlap) for robust retrieval.
- ğŸ’¾ **Local & Private**: Runs offline (except initial model downloads).
- ğŸ–¥ï¸ **CPU-friendly**: GGML via **CTransformers**; GPU optional.
- ğŸ’¡ **Simple UI**: One-line `chainlit run` to chat with your documents.

---

## ğŸ§± Architecture (high level)

```
PDFs (data/)
   â”‚
   â”œâ”€ ingest.py  â†’ load PDFs â†’ split â†’ embed (MiniLM) â†’ FAISS (vectorstore/db_faiss)
   â”‚
   â””â”€ model.py   â†’ retrieve (k=2) â†’ custom prompt â†’ Llama-2 (CTransformers) â†’ Chainlit chat
```

---

## ğŸ“ Project structure

```
.
â”œâ”€ data/                        # put your PDFs here (not tracked by git)
â”œâ”€ vectorstore/
â”‚   â””â”€ db_faiss/               # FAISS index files (auto-generated; git-ignored)
â”œâ”€ ingest.py                    # build the FAISS vector DB from PDFs
â”œâ”€ model.py                     # Chainlit app: RetrievalQA pipeline
â”œâ”€ requirements.txt
â”œâ”€ .env.example                 # optional config (model paths, etc.)
â”œâ”€ .gitignore
â”œâ”€ Dockerfile                   # optional container build
â”œâ”€ Makefile                     # convenience commands
â”œâ”€ scripts/
â”‚   â”œâ”€ ingest_data.sh
â”‚   â””â”€ run_app.sh
â””â”€ tests/
    â””â”€ test_ingest_smoke.py     # simple smoke test for ingestion
```

---

## ğŸš€ Quickstart

### 1) Python & deps
```bash
python -m venv .venv && source .venv/bin/activate   # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
```

### 2) Add documents
Place your PDFs in:
```
data/
  â”œâ”€ doc1.pdf
  â”œâ”€ doc2.pdf
  â””â”€ ...
```

### 3) Build the vector store
```bash
python ingest.py
```

### 4) Run the chatbot (Chainlit)
```bash
chainlit run model.py -w
```

---

## âš™ï¸ Configuration

Environment variables (optional) via `.env`:
```
# .env.example
CTRANSFORMERS_MODEL=TheBloke/Llama-2-7B-Chat-GGML
CTRANSFORMERS_MODEL_TYPE=llama
MAX_NEW_TOKENS=512
TEMPERATURE=0.5
DB_FAISS_PATH=vectorstore/db_faiss
```

You can also point `CTRANSFORMERS_MODEL` to a **local** GGML file to avoid external downloads.

---

## ğŸ“ Prompting & Retrieval

- The custom prompt constrains answers to the retrieved context and asks the model to say â€œI donâ€™t knowâ€ when applicable.
- The retriever uses **k=2** neighbors by default; tweak for recall vs precision.

---

## ğŸ“¦ Dependencies

- LangChain, langchain-community, sentence-transformers, FAISS (CPU), CTransformers, Chainlit, pypdf
- See `requirements.txt` for exact pins.

---

## ğŸ›¡ï¸ Privacy & Licensing

- Your PDFs stay local; embeddings & FAISS index are stored on your machine.
- **Llama-2** usage follows Metaâ€™s license; confirm your intended use complies.
- Do not commit PDF content or FAISS indexes if they contain sensitive data.

---

## ğŸ§ª Testing

```bash
pytest -q
```

Included:
- `tests/test_ingest_smoke.py` checks ingestion doesnâ€™t crash and produces a FAISS store.

---

## ğŸ³ Docker (optional)

```bash
docker build -t rag-lite .
docker run -p 8000:8000 -v $(pwd)/data:/app/data rag-lite
# then open the Chainlit URL from container logs
```

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch
3. Add tests for new behavior
4. Open a PR with a clear description & screenshots (if UI)

---

## ğŸ“œ License

MIT
